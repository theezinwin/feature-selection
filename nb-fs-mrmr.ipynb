{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter-Based Feature Selection, mRMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use a public dataset to perform a selection of features using minimum-Redundancy Maximum-Relevancy.\n",
    "\n",
    "* [Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n",
    "Vol. 27, No. 8, pp.1226-1238, 2005.](http://home.penglab.com/papersall/docpdf/2005_TPAMI_FeaSel.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website http://home.penglab.com/proj/mRMR/ provides a collection of datasets used to test the original algorithm. We will use some of them to test the distributed version of mRMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "filename = \"test_colon_s3.csv\"\n",
    "url = \"http://home.penglab.com/proj/mRMR/\" + filename\n",
    "f = urllib.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_MLlib_ relies on _LabeledPoint_ as data structure to that stores a numerical vector (dense or sparse) and a numerical label. An RDD of LabeledPoint represents the dataset given as input to train or test supervised machine learning models.\n",
    "\n",
    "The function _csv2lp_ transforms each line of the csv file into a LabeledPoint. The second parameter of the function is the number of the first _n_ features to filter, as the downstream analysis would require high amount of memory with the whole set of features. In this example we reduce the initial number of features to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "def csv2lp(x, nf):\n",
    "    xs = [float(y) for y in x.split(\",\")][0:nf]\n",
    "    c  = xs[0]\n",
    "    f  = xs[1:]\n",
    "    \n",
    "    sv = Vectors.sparse(len(f), [(i,j) for i,j in enumerate(f) if j != 0 ])\n",
    "    lp = LabeledPoint(c, sv)\n",
    "    \n",
    "    return lp\n",
    "\n",
    "file_no_header = sc.textFile(filename).filter(lambda x: x[0] != \"c\")  # remove the first line as header of the file.\n",
    "rdd = file_no_header.map(lambda x: csv2lp(x, 20))  # transform the text file into an RDD[LabeledPoint]\n",
    "ncols = rdd.first().features.size  # number of columns (no class) of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(-1.0, (19,[0,4,6,8,11,12,13,15,17,18],[2.0,-2.0,-2.0,2.0,-2.0,-2.0,-2.0,-2.0,-2.0,-2.0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed computations of Mutual Information between class-features pairs, and feature-feature pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we calculate first the mutual information between the every feature and the class vectors (_fscores_), then the mutual information between every pair of features (_ffscores_). Given these intermediate results we proceed into performing the feature selection according to the mRMR algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below we define the functions used during the _map_ and _reduce_ stage of the distributed calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np\n",
    "\n",
    "def meltLPclass(lp):\n",
    "    '''\n",
    "    This function creates a list of k,v tuples, one per each\n",
    "    label-feature combination. 'k' corresponds to the index\n",
    "    of the feature and 'v' corresponds to a tuple of two\n",
    "    elements: value of the label, value of the feature\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lp : LabeledPoint\n",
    "        a point in the feature space with label\n",
    "    '''\n",
    "    label = lp.label\n",
    "    features = lp.features\n",
    "    r = range(features.size)\n",
    "    return [(i, (label, features[i])) for i in r]\n",
    "\n",
    "def meltLPfeatures(lp):\n",
    "    '''\n",
    "    This function creates a list of k,v tuples, one per each\n",
    "    feature-feature combination. 'k' corresponds to the index\n",
    "    of the features and 'v' corresponds to a tuple of two\n",
    "    elements: value of the feature1, value of the feature2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lp : LabeledPoint\n",
    "        a point in the feature space with label\n",
    "    '''\n",
    "    features = lp.features\n",
    "    r = range(features.size)\n",
    "    return [((i, j), (features[i], features[j])) for i in r for j in r if i < j]\n",
    "\n",
    "def mi(x):\n",
    "    '''\n",
    "    This function calculates the Mutual information between two discrete variables.\n",
    "    It relies on the 'adjusted_mutual_info_score' function available in the\n",
    "    sklearn package.\n",
    "    \n",
    "    Doc: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tuple\n",
    "        x[0] is a scalar value (or a tuple), representing the index(es) of the feature(s)\n",
    "        x[1] is a pyspark.resultiterable.ResultIterable object\n",
    "    '''\n",
    "    idx = x[0]\n",
    "    values = list(x[1])\n",
    "    \n",
    "    l = list(values)\n",
    "    v1, v2 = zip(*values)\n",
    "    res = normalized_mutual_info_score(v1, v2)\n",
    "    \n",
    "    return (idx, res)\n",
    "\n",
    "def miCont(x):\n",
    "    '''\n",
    "    This function calculates the Mutual information between two continuous variables.\n",
    "    It relies on the 'mutual_info_regression' function available in the\n",
    "    sklearn package.\n",
    "    \n",
    "    Doc: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tuple\n",
    "        x[0] is a scalar value (or a tuple), representing the index(es) of the feature(s)\n",
    "        x[1] is a pyspark.resultiterable.ResultIterable object\n",
    "    '''\n",
    "    idx = x[0]\n",
    "    values = list(x[1])\n",
    "    \n",
    "    l = list(values)\n",
    "    v1, v2 = zip(*values)\n",
    "    V1 = np.array(v1).reshape(len(v1),1)  # 1-column matrix layout transformation\n",
    "    res = mutual_info_regression(V1, v2, discrete_features=False, random_state=42)[0]  # random_state is set to provide deterministic results\n",
    "    \n",
    "    return (idx, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our RDD (_rdd_) that represents our dataset,\n",
    "- the _flatMap_ operation iterates over every single instance and produce intermediate tuples containing the values of the pair label-feature and the feature index. We need the feature index to be the key of the tuple, so we can group every tuple regarding such feature in the Reducers.\n",
    "- the _groupByKey_ operation sort and gather tuples having the same key in the Reducers (one Reducer per each key)\n",
    "- the _map_ operation of feature _j_ has been provided with all the data label-feature of the feature _j_. That is the vectors of the label and the feature _j_. Having such data in one place, the _mi_ function can calculate the mutual information. You can plug in other (custom) functions to assess the association of the two variables, such as _miCont_.\n",
    "- the _collect_ operation with collect all the resulting data in the spark driver process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fscores = rdd.flatMap(meltLPclass).groupByKey().map(mi).collect()\n",
    "fdict = dict(fscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.038187438441575725,\n",
       " 2: 0.030097363097944068,\n",
       " 4: 0.045972729310487084,\n",
       " 6: 0.007314754207585962,\n",
       " 8: 0.038187438441575725}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(fscores[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our RDD (_rdd_) that represents our dataset,\n",
    "- the _flatMap_ operation iterates over every single instance and produce intermediate tuples containing the values of the pair feature-feature and the feature indexes. We need the feature indexes to be the key of the tuple, so we can group every tuple regarding the pair of features in the Reducers. This is our distributed way of computing each element of the mutual information matrix.\n",
    "- Following steps are described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ffscores = rdd.flatMap(meltLPfeatures).groupByKey().map(mi).collect()\n",
    "ffdict = dict(ffscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 12): 0.04549685587602393,\n",
       " (2, 18): 0.048374754181886546,\n",
       " (3, 15): 0.10831458954134202,\n",
       " (4, 6): 0.10776993745903882,\n",
       " (6, 10): 0.089789535509841714}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(ffscores[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minimum-Redundancy Maximum-Relevancy (mRMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the mutual information matrixes of class-feature and feature-features, we select the top _nfs_ features according to the mRMR algorithm. _selFidx_ stores the indexes of the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfs = 5  # number of feature to select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 0.1045852767092034),\n",
       " (15, 0.069298782833987979),\n",
       " (2, -0.00604816180222233),\n",
       " (16, -0.040546956404271237),\n",
       " (8, -0.04021388673654136)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute(cidx, fdict, ffdict, selFidx):\n",
    "    mrmrC = fdict[cidx]\n",
    "    mrmrF = 0\n",
    "    for sidx in selFidx:\n",
    "        if cidx < sidx:\n",
    "            mrmrF = mrmrF + ffdict[(cidx, sidx)]\n",
    "        else:\n",
    "            mrmrF = mrmrF + ffdict[(sidx, cidx)]\n",
    "    if len(selFidx) > 0:\n",
    "        return mrmrC - (mrmrF / len(selFidx))\n",
    "    else:\n",
    "        return mrmrC\n",
    "\n",
    "selFidx = []\n",
    "selFscore = []\n",
    "canFidx = range(ncols)\n",
    "out = []\n",
    "for i in range(nfs):\n",
    "    mrmr_i = [(cidx, compute(cidx, fdict, ffdict, selFidx)) for cidx in canFidx]\n",
    "    mrmr_i.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    best_feature_i = mrmr_i[0]\n",
    "    selFidx.append(best_feature_i[0])\n",
    "    selFscore.append(best_feature_i[1])\n",
    "    canFidx.remove(best_feature_i[0])\n",
    "\n",
    "zip(selFidx, selFscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to reduce the dimensionality of _rdd_ according to the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "def reduceLP(lp, fsIdx):\n",
    "    label = lp.label\n",
    "    features = lp.features\n",
    "    v = [features[i] for i in fsIdx]\n",
    "    return LabeledPoint(label, Vectors.dense(v))\n",
    "\n",
    "rddFS = rdd.map(lambda x: reduceLP(x, selFidx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(-1.0, [0.0,-2.0,0.0,0.0,2.0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddFS.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food for thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. does this algorithm scale with the number of instances? That is, given 100M instances instead of the current 1K, will this code work or will it crash?\n",
    "2. what about the same question above concerning the number of feature instead?\n",
    "3. for this feature selection, do we need to rely on the LabeledPoint data structure? Can we use another (simpler) data structure?\n",
    "4. can I directly calculate the mutual information in _map_ phase instead of going through the _map_ and _reduce_ phases? Why?\n",
    "5. Can you calcuate the number of tuples produced at the _flatMap_ operation? Can you estimate the ratio between the size of such intermediate results and the original dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
